{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b9dd028",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Omit this step if you already have SparkContext available\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d1edeaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pyspark.sql import DataFrame\n",
    "import pyspark.sql.functions as sf\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "\n",
    "def generate_random_uniform_df(nrows, ncols, seed=1):\n",
    "    df = spark.range(nrows).select(sf.col(\"id\"))\n",
    "    df = df.select('*', *(F.rand(seed).alias(\"_\"+str(target)) for target in range(ncols)))\n",
    "    return df.drop(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4fb2aa14",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ea5c5fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.129:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x112ad7820>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bce153",
   "metadata": {},
   "source": [
    "# Extract Exact size of a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0efe1ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_small = generate_random_uniform_df(10000, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e55b8838",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------\n",
      " _0  | 0.6363787615254752 \n",
      " _1  | 0.6363787615254752 \n",
      " _2  | 0.6363787615254752 \n",
      " _3  | 0.6363787615254752 \n",
      " _4  | 0.6363787615254752 \n",
      " _5  | 0.6363787615254752 \n",
      " _6  | 0.6363787615254752 \n",
      " _7  | 0.6363787615254752 \n",
      " _8  | 0.6363787615254752 \n",
      " _9  | 0.6363787615254752 \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_small.show(n=1, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "63a9eb0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_small.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5a8bc808",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_size_in_bytes_exact(df: DataFrame):\n",
    "    \"\"\"\n",
    "    Calculates the exact size in memory of a DataFrame by caching it and accessing the optimized plan\n",
    "\n",
    "    NOTE: BE CAREFUL WITH THIS FUNCTION BECAUSE IT WILL CACHE ALL THE DATAFRAME!!! IF YOUR DATAFRAME IS\n",
    "    TOO BIG USE `estimate_df_size_in_bytes`!!\n",
    "\n",
    "    :param df: A pyspark DataFrame\n",
    "    :return: The exact size in bytes\n",
    "    \"\"\"\n",
    "    df = df.cache().select(\n",
    "        df.columns\n",
    "    )  # Just force the Spark planner to add the Cache op to the plan\n",
    "    logging.info(f\"Number of rows in the input DataFrame: {df.count()}\")\n",
    "    size_in_bytes = df._jdf.queryExecution().optimizedPlan().stats().sizeInBytes()\n",
    "    df.unpersist(blocking=True)\n",
    "    return size_in_bytes\n",
    "\n",
    "def df_size_in_bytes_approximate(df: DataFrame, sample_perc: float = 0.05):\n",
    "    \"\"\"\n",
    "    This method takes a sample of the input DataFrame (`sample_perc`) and applies `df_size_in_bytes_exact`\n",
    "    method to it. After it calculates the exact size of the sample, it extrapolates the total size.\n",
    "\n",
    "    :param df: A pyspark DataFrame\n",
    "    :param sample_perc: The percentage of the DataFrame to sample. By default, a 5 %\n",
    "    :return: The estimated size in bytes\n",
    "    \"\"\"\n",
    "    sample_size_in_bytes = df_size_in_bytes_exact(df.sample(sample_perc))\n",
    "    return sample_size_in_bytes / sample_perc\n",
    "\n",
    "\n",
    "def add_partition_id_column(df: DataFrame):\n",
    "    return df.withColumn(\"partition_id\", sf.spark_partition_id())\n",
    "\n",
    "\n",
    "def get_partition_count(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Gets the number of registers per partition. This method is useful if we are trying to determine if some\n",
    "    partition is skewed.\n",
    "\n",
    "    :return: A DataFrame containing `partition_id` and `count` columns\n",
    "    \"\"\"\n",
    "    return add_partition_id_column(df).groupBy(\"partition_id\").count()\n",
    "\n",
    "\n",
    "def add_salt_column(df: DataFrame, skew_factor: int):\n",
    "    \"\"\"\n",
    "    Adds a salt column to a DataFrame. We will be using this salt column when we are trying to perform\n",
    "    join, groupBy, etc. operations into a skewed DataFrame. The idea is to add a random column and use\n",
    "    the original keys + this salted key to perform the operations, so that we can avoid data skewness and\n",
    "    possibly, OOM errors.\n",
    "\n",
    "    :param df: A Pyspark DataFrame\n",
    "    :param skew_factor: The skew factor. For example, if we set this value to 3, then the salted column will\n",
    "        be populated by the elements 0, 1 and 2, extracted from a uniform probability distribution.\n",
    "    :return: The original DataFrame with a `salt_id` column.\n",
    "    \"\"\"\n",
    "    return df.withColumn(\"salt_id\", (sf.rand() * skew_factor).cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "93673c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "800000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_size_in_bytes_exact(df_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "da7634e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "812800.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_size_in_bytes_approximate(df_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e778a5c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|partition_id|count|\n",
      "+------------+-----+\n",
      "|           0| 1250|\n",
      "|           1| 1250|\n",
      "|           2| 1250|\n",
      "|           3| 1250|\n",
      "|           4| 1250|\n",
      "|           5| 1250|\n",
      "|           6| 1250|\n",
      "|           7| 1250|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_partition_count(df_small).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "18dd6db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_small_with_salt = add_salt_column(df_small, skew_factor=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3ba20880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|salt_id|\n",
      "+-------+\n",
      "|      1|\n",
      "|      0|\n",
      "|      1|\n",
      "|      1|\n",
      "|      0|\n",
      "|      1|\n",
      "|      0|\n",
      "|      0|\n",
      "|      0|\n",
      "|      1|\n",
      "|      0|\n",
      "|      1|\n",
      "|      0|\n",
      "|      0|\n",
      "|      0|\n",
      "|      1|\n",
      "|      0|\n",
      "|      1|\n",
      "|      1|\n",
      "|      1|\n",
      "+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_small_with_salt.select(\"salt_id\").show(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "440e3216",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_small_with_salt_and_partition_id = add_salt_column(add_partition_id_column(df_small), skew_factor=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "05b7e819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+\n",
      "|partition_id|salt_id|\n",
      "+------------+-------+\n",
      "|           0|      0|\n",
      "|           0|      0|\n",
      "|           0|      1|\n",
      "|           0|      0|\n",
      "|           0|      0|\n",
      "|           0|      0|\n",
      "|           0|      0|\n",
      "|           0|      0|\n",
      "|           0|      0|\n",
      "|           0|      1|\n",
      "+------------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_small_with_salt_and_partition_id.select(\"partition_id\", \"salt_id\").show(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4a8c6ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+-----+\n",
      "|partition_id|salt_id|count|\n",
      "+------------+-------+-----+\n",
      "|           0|      0|  638|\n",
      "|           0|      1|  612|\n",
      "|           1|      0|  586|\n",
      "|           1|      1|  664|\n",
      "|           2|      1|  635|\n",
      "|           2|      0|  615|\n",
      "|           3|      1|  603|\n",
      "|           3|      0|  647|\n",
      "|           4|      0|  653|\n",
      "|           4|      1|  597|\n",
      "|           5|      0|  618|\n",
      "|           5|      1|  632|\n",
      "|           6|      1|  600|\n",
      "|           6|      0|  650|\n",
      "|           7|      1|  637|\n",
      "|           7|      0|  613|\n",
      "+------------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_small_with_salt_and_partition_id.groupBy(\"partition_id\", \"salt_id\").count().show(n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4b1a2ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|partition_id|count|\n",
      "+------------+-----+\n",
      "|           0| 1250|\n",
      "|           1| 1250|\n",
      "|           2| 1250|\n",
      "|           3| 1250|\n",
      "|           4| 1250|\n",
      "|           5| 1250|\n",
      "|           6| 1250|\n",
      "|           7| 1250|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_partition_count(df_small_with_salt_and_partition_id).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a59eb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
